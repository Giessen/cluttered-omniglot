{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "OMNIGLOT_DATA = os.path.join(os.getcwd(), 'omniglot/')\n",
    "DATASET_DIR = os.path.join(os.getcwd(), 'cluttered_omniglot/')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "\n",
    "import dataset_utils\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dataset_utils.DatasetGeneratorConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Omniglot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_glots(glots):\n",
    "    reordered_glots=[]\n",
    "    for alph in range(len(glots)):\n",
    "        for char in range(len(glots[alph])):\n",
    "            reordered_glots.append(glots[alph][char])\n",
    "    \n",
    "    return reordered_glots\n",
    "\n",
    "#Load glots from pickle file\n",
    "path = OMNIGLOT_DATA\n",
    "\n",
    "#Train split\n",
    "with open(path + 'glots_train.pickle', 'rb') as fp:\n",
    "    glots_train = pickle.load(fp)\n",
    "    glots_train = reorder_glots(glots_train)\n",
    "    \n",
    "#Evaluation split\n",
    "with open(path + 'glots_eval.pickle', 'rb') as fp:\n",
    "    glots_eval = pickle.load(fp)\n",
    "    glots_eval = reorder_glots(glots_eval)\n",
    "    \n",
    "#Test split\n",
    "with open(path + 'glots_test.pickle', 'rb') as fp:\n",
    "    glots_test = pickle.load(fp)\n",
    "    glots_test = reorder_glots(glots_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checksums = [[b'\\xeb\\xce\\x01\\x9a\\xe7f\\xbb\\xaau;\\xcd\\xf1-\\xea\\xf9\\xe5', \n",
    "              b'\\x9f1\\x0e\\x90\\xc5r\\x11\\xc9\\xbb#\\xd0\\xa9\\xc6&\\xd6\\xdd', \n",
    "              b\"!h\\x10\\xeb'\\xad\\xe4\\x81\\xf2\\xe0\\xd8cun\\x11\\xff\", \n",
    "              b'#\\xacSz\\xf1\\xdc\\xcc\\xd9{\\xaf\\xf0pk\\xf4N\\xd1'], \n",
    "             [b'C\\x1a\\xbf0\\x80@\\xc1Hr(*\\xc36\\x89R\\xfa', \n",
    "              b'\\xa13\\xe2\\xf8\\xbeX\\x97P\\x8e\\xb87\\xc4\\xabH\\xdc\\xf5', \n",
    "              b';\\x9apTq\\n\\xbf`CdR\\xe8b\\x85\\xc4\\xd9', \n",
    "              b'El\\xd0\\xfa\\xc0\\x9c<L\\x1f\\xf1o\\xdai\\x05\\x14\\x89'], \n",
    "             [b'\\x08W\\xafw\\xa9\\xa8\\xea\\xbd\\xfa)\\x14\\xa4\\xb4\\xf0c\\x13', \n",
    "              b'~\\x8c\\x84\\x06`\\x8bA\\xb3m\\xfb\\xd4\\xe4\\x00\\xb2\\x83\\x1a', \n",
    "              b'\\x0cv\\xa1.B\\xfc\\x16F\\xb8u\\xa4\\xbap\\x07\\xbc\\xe9', \n",
    "              b'\\x02<\\xe5\\x1eg\\xf7WWEr4]\\xc2\\xb1|='], \n",
    "             [b'/\\xe56\\xff\\xec\\xb2}\\xa9\\xe1\\xec\\xa1\"\\xdcV\\x1dX', \n",
    "              b'\\x0c\\xc3\\xd6N\\xcc\\x1e\\xec\\x03\\x8a[J;[\\n2|', \n",
    "              b'\\xb5\\xb7\\x1di\\xb1\\xbb\\xed\\x7f\\x08\\xfc\\x94.\\xb4\\xc6\\xdcZ', \n",
    "              b'\\xe5\\xe2\\x9a\\x93\\xfb\\xf0ptJW\\xb2\\x9fv\\xc7`$'], \n",
    "             [b'\\xff\\xec\\x121\\xe4\\xd0K\\xdd\\x8c\\x08\\xf0.N\\xc2%\\xe0', \n",
    "              b'\\x12\\xe5X\\xe8_\\x92!7n\\xdf\\x94,KS\\x0f\\x7f', \n",
    "              b\"T\\x9f\\xba\\xe4\\xa1hxq'\\xf8\\x91X\\x93\\xedC\\xe5\", \n",
    "              b'^\\x1f@\\\\\\xacUrX\\xadR\\n\\xb5\\xcd\\xbb\\xa6)'], \n",
    "             [b'\\xd0\\xa4\\xbe\\x038\\xca\\xf17r\\xacg\\xc3w5\\xea\\xa0', \n",
    "              b'\\xd9W\\xe5\\xf3\\xcc\\x03w\\xebL\\xe6;\\tl]\\xf2\\xfb', \n",
    "              b'\\x90\\x93\\x15\\xc9\\xbb\\xcd)\\x96o\\xf5\\x18\"|\\r2X', \n",
    "              b'\\x90\\xf6!Ts\\x1b7\\xbb\\xf0\\xc4\\x04\\xcd\\xcd\\xe4\\xc3\\xdb'], \n",
    "             [b'*\\x88\\x88@\\xc8\\x11\\xf4+\\xab\\x05\\xfflE\\xc1\\xcd\\x9b', \n",
    "              b'\\x84\\x02\\xf3j\\xf9\\x99\\xc0\\x03\\x9a\\x05\\xd1\\x02\\x97~\\xbd\\xaa', \n",
    "              b'\\xbb\\x12\\x9f\\xc4\\t\\xdd\\xa2\\xde\\xeeW\\x1b\\xf58\\xdb\\x8f\\xdc', \n",
    "              b\"\\x0fN\\xc5\\x87\\x97\\xda\\xd6\\x84x\\xc6L'\\x177Zz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate indicidual datasets for each clutter level\n",
    "nname =  [4, 8, 16, 32, 64, 128, 256]\n",
    "nchars = [3, 7, 15, 31, 63, 127, 255]\n",
    "# Generate one dataset for all clutter levels\n",
    "# nname =  ['mix']\n",
    "# nglots = [[3, 7, 15, 31, 63, 127, 255]]\n",
    "\n",
    "for i in range(len(nname)):\n",
    "    # Set and print saving directory\n",
    "    dset_dir = DATASET_DIR + '{}_characters/'.format(nname[i])\n",
    "    print('')\n",
    "    print(dset_dir)\n",
    "\n",
    "    # Set number of images per parallel job\n",
    "    config.JOBLENGTH = 2000\n",
    "    # Set number of distractors\n",
    "    config.DISTRACTORS = nchars[i]\n",
    "    \n",
    "    ### Generate training set ###\n",
    "    \n",
    "    # Set dataset split\n",
    "    config.DRAWER_SPLIT = 'train'\n",
    "    config.set_drawer_split()\n",
    "    # Define number of train images\n",
    "    dataset_size = 2000000\n",
    "    # Choose training alphabets\n",
    "    glots = glots_train\n",
    "    # Set path\n",
    "    path = dset_dir + 'train/'\n",
    "    # Set a fixed seed\n",
    "    seed_train = 2209944264\n",
    "\n",
    "    # Generate dataset\n",
    "    print('Generating dataset train/')\n",
    "    dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=seed_train, save=True)\n",
    "    print('')\n",
    "\n",
    "\n",
    "    ### Generate evaluation and test sets ###\n",
    "\n",
    "    # Set dataset split\n",
    "    config.DRAWER_SPLIT = 'val'\n",
    "    config.set_drawer_split()\n",
    "    # Define number of val/test images\n",
    "    dataset_size = 10000\n",
    "\n",
    "    #Generate evaluation set on train characters\n",
    "    seed_val_train = 4020197800\n",
    "    glots = glots_train\n",
    "    path = dset_dir + 'val-train/'\n",
    "    print('Generating dataset val-train/')\n",
    "    dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=seed_val_train, save=True, checksum=checksums[i][0])\n",
    "    print('')\n",
    "    \n",
    "    #Generate test set on train characters\n",
    "    seed_test_train = 1665765955\n",
    "    glots = glots_train\n",
    "    path = dset_dir + 'test-train/'\n",
    "    print('Generating dataset val-train/')\n",
    "    dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=seed_test_train, save=True, checksum=checksums[i][1])\n",
    "    print('')\n",
    "\n",
    "    #Generate evaluation set on one-shot characters\n",
    "    seed_val_one_shot = 3755213170\n",
    "    glots = glots_eval\n",
    "    path = dset_dir + 'val-one-shot/'\n",
    "    print('Generating dataset val-one-shot/')\n",
    "    dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=seed_val_one_shot, save=True, checksum=checksums[i][2])\n",
    "    print('')\n",
    "\n",
    "    #Generate test set on one-shot characters\n",
    "    seed_test_one_shot = 2301871561\n",
    "    glots = glots_test\n",
    "    path = dset_dir + 'test-one-shot/'\n",
    "    print('Generating dataset test-one-shot/')\n",
    "    dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=seed_test_one_shot, save=True, checksum=checksums[i][3])\n",
    "    print('')\n",
    "\n",
    "print('')\n",
    "print('All Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show some randomly generated example images\n",
    "\n",
    "# Define number of images\n",
    "dataset_size = 5\n",
    "\n",
    "# Choose training alphabets\n",
    "glots = glots_train\n",
    "# Set number of images per parallel job\n",
    "config.JOBLENGTH = 1\n",
    "# Set number of distractors\n",
    "config.DISTRACTORS = 31\n",
    "# Set dataset split\n",
    "config.DRAWER_SPLIT = 'train'\n",
    "config.set_drawer_split()\n",
    "\n",
    "# set path if images should be saved\n",
    "dset_dir = DATASET_DIR + '{}_characters/'.format(config.DISTRACTORS + 1)\n",
    "path = DATASET_DIR + 'visualize/'\n",
    "\n",
    "# Run visualization\n",
    "dataset_utils.generate_dataset(path, dataset_size, glots, config, seed=None, save=False, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
